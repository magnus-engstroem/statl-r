---
title: "Compulsory Exercise 2: Title (give your project an informative title)"
author:
- "Einride B. Osland" \#1.
- "Magnus Engstrom" \#2.
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---

```{r setup, include=FALSE}
library(knitr)
# Feel free to change the setting as you see fit
knitr::opts_chunk$set(echo = TRUE,
                      tidy = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      strip.white = TRUE,
                      prompt = FALSE,
                      cache = TRUE,
                      size = "scriptsize",
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = "center")

```

```{r, eval=TRUE, echo=FALSE}
library("knitr")
library("rmarkdown")
library(gbm)
library(caret)
library(ggplot2)
library(randomForest)
library(GGally)
```

<!--  Etc (load all packages needed). -->

## Introduction: Scope and purpose of your project

## Abstract (max. 350 words) (5 points)

In this project we are aiming to predict customer satisfaction of people who rent Airbnb apartments. This is going to help future customers to choose where to rent in order to have a better experience. It will also help Airbnb filter out apartments that tend to not give good satisfaction, and thus drive away customers, and instead recommend places that tend to make people return.

Our dataset consists of the Amsterdam weekdays data from the kaggle website. (https://www.kaggle.com/datasets/thedevastator/airbnb-prices-in-european-cities) 
The data consists of 1103 observations of 20 variables, those being:  "X","realSum" "room_type","room_shared",room_private","person_capacity","host_is_superhost"          "multi","biz","cleanliness_rating","guest_satisfaction_overall", "bedrooms","dist","metro_dist","attr_index","attr_index_norm","rest_index","rest_index_norm",lng" and "lat".



## Descriptive data analysis/statistics

```{r load data}

Adam_w_data <- data.frame(read.csv("amsterdam_weekdays.csv"))
Adam_w_data <- subset(Adam_w_data, select = -X)
Adam_w_data$room_type <- as.factor(Adam_w_data$room_type)
Adam_w_data$room_shared <- as.factor(Adam_w_data$room_shared)
Adam_w_data$room_private <- as.factor(Adam_w_data$room_private)
Adam_w_data$host_is_superhost <- as.factor(Adam_w_data$host_is_superhost)
summary(Adam_w_data)
```
When studying the data one can observe that there are multiple variables that 


```{r}
Adam_w_data

```

## Methods

## Results and interpretation

## Summary



```{r}

Adam_w_data["guest_satisfaction_overall"]

nums <- unlist(lapply(Adam_w_data, is.numeric), use.names = FALSE)  


ggpairs(Adam_w_data[,nums])

ggpairs(Adam_w_data[,-nums])

```


### Principle component analysis

hei


```{r}
df<-Adam_w_data


non_numeric_vars <- sapply(df, function(x) !is.numeric(x))

numeric_df <- df[, !non_numeric_vars]


pca_result <- prcomp(numeric_df, scale. = TRUE)

pca_components <- pca_result$x


variance_explained <- pca_result$sdev^2
```


```{r}

plot(pca_result)


plot(pca_components[, 1], pca_components[, 2], pch = 16, col = "blue", xlab = "PC1", ylab = "PC2")

```
```{r Random forest}

set.seed(123)
colnames(Adam_w_data)
mean(Adam_w_data[,"guest_satisfaction_overall"])

train_indices <- sample(nrow(Adam_w_data), 0.7 * nrow(Adam_w_data_df))
train_data <- Adam_w_data[train_indices, ]
test_data <- Adam_w_data[-train_indices, ]

# Train the Random Forest model
rf_model <- randomForest(guest_satisfaction_overall ~ ., data = train_data)

# Print the model
print(rf_model)

# Make predictions on the testing set
predictions <- predict(rf_model, test_data)


# Evaluate the model using Mean Absolute Error (MAE)
mae <- mean(abs(predictions - test_data$guest_satisfaction_overall))
cat("Mean Absolute Error (MAE):", mae, "\n")

# Evaluate the model using Mean Squared Error (MSE)
mse <- mean((predictions - test_data$guest_satisfaction_overall)^2)
cat("Mean Squared Error (MSE):", mse, "\n")

# Evaluate the model using Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)
cat("Root Mean Squared Error (RMSE):", rmse, "\n")


```



```{r Gbm}
Adam_w_gbm <- gbm(formula = guest_satisfaction_overall ~., 
                  data = Adam_w_data,
                  distribution = "gaussian",
                  n.trees = 300,
                  shrinkage = 0.1,
                  interaction.depth = 5,
                  n.minobsinnode = 10,
                  bag.fraction = 1,
                  cv.folds = 5,
                  )

gbm.perf(Adam_w_gbm, method = "cv")
summary.gbm(Adam_w_gbm)




summary(Adam_w_lm)





```


```
```{r}


Kfoldlm <- function(K){
  folds <- 1:nrow(Adam_w_data)%%K
  errors = rep(0, K)
  
  for (i in 0:(K-1)){
    train <- subset(Adam_w_data, folds != i)
    test <- subset(Adam_w_data, folds == i)
    Adam_lm <- lm(guest_satisfaction_overall ~., data = train)
    predicions <- predict(Adam_lm, test, type = "response")
    Squared_errors <- (test["guest_satisfaction_overall"] - predictions)^2
    errors[i+1] <- sum(Squared_errors)
  }
  
  MSE <- sum(errors)/nrow(Adam_w_data)
  return (MSE)
}

Kfoldlm(10)






```

